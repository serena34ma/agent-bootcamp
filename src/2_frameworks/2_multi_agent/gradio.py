"""Multi-agent Planner-Researcher Setup via OpenAI Agents SDK.

Log traces to LangFuse for observability and evaluation.
"""

import asyncio
import contextlib
import logging
import signal
import sys

import agents
import gradio as gr
from dotenv import load_dotenv
from gradio.components.chatbot import ChatMessage
from openai import AsyncOpenAI
from pydantic import BaseModel

from src.utils import (
    AsyncWeaviateKnowledgeBase,
    Configs,
    get_weaviate_async_client,
    oai_agent_items_to_gradio_messages,
    pretty_print,
    setup_langfuse_tracer,
)
from src.utils.langfuse.shared_client import langfuse_client


load_dotenv(verbose=True)

logging.basicConfig(level=logging.INFO)


PLANNER_INSTRUCTIONS = """\
You are a research planner. \
Given a user's query, produce a list of search terms that can be used to retrieve
relevant information from a knowledge base to answer the question. \
As you are not able to clarify from the user what they are looking for, \
your search terms should be broad and cover various aspects of the query. \
Output between 5 to 10 search terms to query the knowledge base. \
Note that the knowledge base is a Wikipedia dump and cuts off at May 2025.
"""

RESEARCHER_INSTRUCTIONS = """\
You are a research assistant with access to a knowledge base. \
Given a potentially broad search term, use the search tool to \
retrieve relevant information from the knowledge base and produce a short
summary of at most 300 words.
"""

WRITER_INSTRUCTIONS = """\
You are an expert at synthesizing information and writing coherent reports. \
Given a user's query and a set of search summaries, synthesize these into a \
coherent report (at least a few paragraphs long) that answers the user's question. \
Do not make up any information outside of the search summaries.
"""


class SearchItem(BaseModel):
    """A single search item in the search plan."""

    # The search term to be used in the knowledge base search
    search_term: str

    # A description of the search term and its relevance to the query
    reasoning: str


class SearchPlan(BaseModel):
    """A search plan containing multiple search items."""

    search_steps: list[SearchItem]

    def __str__(self) -> str:
        """Return a string representation of the search plan."""
        return "\n".join(
            f"Search Term: {step.search_term}\nReasoning: {step.reasoning}\n"
            for step in self.search_steps
        )


class ResearchReport(BaseModel):
    """Model for the final report generated by the writer agent."""

    # The summary of the research findings
    summary: str

    # full report text
    full_report: str


async def _create_search_plan(planner_agent: agents.Agent, query: str) -> SearchPlan:
    """Create a search plan using the planner agent."""
    with langfuse_client.start_as_current_span(
        name="create_search_plan", input=query
    ) as planner_span:
        response = await agents.Runner.run(planner_agent, input=query)
        search_plan = response.final_output_as(SearchPlan)
        planner_span.update(output=search_plan)

    return search_plan


async def _generate_final_report(
    writer_agent: agents.Agent, search_results: list[str], query: str
) -> agents.RunResult:
    """Generate the final report using the writer agent."""
    input_data = f"Original question: {query}\n"
    input_data += "Search summaries:\n" + "\n".join(
        f"{i + 1}. {result}" for i, result in enumerate(search_results)
    )

    with langfuse_client.start_as_current_span(
        name="generate_final_report", input=input_data
    ) as writer_span:
        response = await agents.Runner.run(writer_agent, input=input_data)
        writer_span.update(output=response.final_output)

    return response


async def _cleanup_clients() -> None:
    """Close async clients."""
    await async_weaviate_client.close()
    await async_openai_client.close()


def _handle_sigint(signum: int, frame: object) -> None:
    """Handle SIGINT signal to gracefully shutdown."""
    with contextlib.suppress(Exception):
        asyncio.get_event_loop().run_until_complete(_cleanup_clients())
    sys.exit(0)


async def _main(question: str, gr_messages: list[ChatMessage]):
    planner_agent = agents.Agent(
        name="Planner Agent",
        instructions=PLANNER_INSTRUCTIONS,
        model=agents.OpenAIChatCompletionsModel(
            model="gemini-2.5-flash", openai_client=async_openai_client
        ),
        output_type=SearchPlan,
    )
    research_agent = agents.Agent(
        name="Research Agent",
        instructions=RESEARCHER_INSTRUCTIONS,
        tools=[agents.function_tool(async_knowledgebase.search_knowledgebase)],
        model=agents.OpenAIChatCompletionsModel(
            model="gemini-2.5-flash-lite-preview-06-17",
            openai_client=async_openai_client,
        ),
        model_settings=agents.ModelSettings(tool_choice="required"),
    )
    writer_agent = agents.Agent(
        name="Writer Agent",
        instructions=WRITER_INSTRUCTIONS,
        model=agents.OpenAIChatCompletionsModel(
            model="gemini-2.5-flash", openai_client=async_openai_client
        ),
        output_type=ResearchReport,
    )

    gr_messages.append(ChatMessage(role="user", content=question))
    yield gr_messages

    with langfuse_client.start_as_current_span(
        name="Multi-Agent-Trace", input=question
    ) as agents_span:
        # Create a search plan
        search_plan = await _create_search_plan(planner_agent, question)
        gr_messages.append(
            ChatMessage(role="assistant", content=f"Search Plan:\n{search_plan}")
        )
        pretty_print(gr_messages)
        yield gr_messages

        # Execute the search plan
        search_results = []
        for step in search_plan.search_steps:
            with langfuse_client.start_as_current_span(
                name="execute_search_step", input=step.search_term
            ) as search_span:
                response = await agents.Runner.run(
                    research_agent, input=step.search_term
                )
                search_result: str = response.final_output
                search_span.update(output=search_result)

            search_results.append(search_result)
            gr_messages += oai_agent_items_to_gradio_messages(response.new_items)
            yield gr_messages

        # Generate the final report
        writer_agent_response = await _generate_final_report(
            writer_agent, search_results, question
        )
        agents_span.update(output=writer_agent_response.final_output)

        report = writer_agent_response.final_output_as(ResearchReport)
        gr_messages.append(
            ChatMessage(
                role="assistant",
                content=f"Summary:\n{report.summary}\n\nFull Report:\n{report.full_report}",
            )
        )
        pretty_print(gr_messages)
        yield gr_messages


if __name__ == "__main__":
    configs = Configs.from_env_var()
    async_weaviate_client = get_weaviate_async_client(
        http_host=configs.weaviate_http_host,
        http_port=configs.weaviate_http_port,
        http_secure=configs.weaviate_http_secure,
        grpc_host=configs.weaviate_grpc_host,
        grpc_port=configs.weaviate_grpc_port,
        grpc_secure=configs.weaviate_grpc_secure,
        api_key=configs.weaviate_api_key,
    )
    async_knowledgebase = AsyncWeaviateKnowledgeBase(
        async_weaviate_client,
        collection_name="enwiki_20250520",
    )

    async_openai_client = AsyncOpenAI()
    setup_langfuse_tracer()

    with gr.Blocks(title="OAI Agent SDK - Multi-agent") as app:
        chatbot = gr.Chatbot(type="messages", label="Agent", height=600)
        chat_message = gr.Textbox(lines=1, label="Ask a question")
        chat_message.submit(_main, [chat_message, chatbot], [chatbot])

    signal.signal(signal.SIGINT, _handle_sigint)

    try:
        app.launch(server_name="0.0.0.0")
    finally:
        asyncio.run(_cleanup_clients())
